import io
import pandas as pd
import streamlit as st
from sqlalchemy import create_engine, text

st.set_page_config(page_title="KT dashboard", layout="wide")

# ---------------- DB ----------------
@st.cache_resource
def get_engine():
    return create_engine(
        st.secrets["DATABASE_URL"],
        pool_pre_ping=True,
        future=True
    )

engine = get_engine()

# ---------------- Helpers ----------------
def read_csv_ru(f):
    # utf-8-sig Ğ»ĞµÑ‡Ğ¸Ñ‚ BOM Ğ² Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ñ…
    return pd.read_csv(f, sep=";", encoding="utf-8-sig", engine="python")

def pick_col(df: pd.DataFrame, candidates: list[str]) -> str:
    # 1) Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»Ğµ strip
    stripped_map = {c.strip(): c for c in df.columns}
    for cand in candidates:
        if cand in stripped_map:
            return stripped_map[cand]

    # 2) case-insensitive ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ
    lowered = {c.lower().strip(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower().strip()
        if key in lowered:
            return lowered[key]

    raise KeyError(
        f"ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ° {candidates}. "
        f"Ğ¤Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸: {list(df.columns)}"
    )

def copy_df_to_table(conn, df: pd.DataFrame, table: str):
    """
    COPY df -> table (Postgres) Ñ‡ĞµÑ€ĞµĞ· psycopg2 copy_expert.
    conn: SQLAlchemy Connection (Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ engine.begin()).
    """
    raw = conn.connection  # psycopg2 connection
    cur = raw.cursor()

    buf = io.StringIO()
    # Ğ‘ĞµĞ· header, Ğ¸Ğ½Ğ°Ñ‡Ğµ COPY Ğ±ÑƒĞ´ĞµÑ‚ Ğ¿Ñ‹Ñ‚Ğ°Ñ‚ÑŒÑÑ Ğ²ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº ĞºĞ°Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    df.to_csv(buf, index=False, header=False)
    buf.seek(0)

    cols = ",".join(df.columns)
    sql = f"COPY {table} ({cols}) FROM STDIN WITH (FORMAT CSV)"

    cur.copy_expert(sql, buf)
    cur.close()

# ---------------- Loaders ----------------
def load_clicks(file):
    st.write("ğŸ§© start_load_clicks")

    # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ ĞºÑƒÑĞºĞ°Ğ¼Ğ¸ â€” Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ ÑƒĞ±Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ½Ğ° Streamlit Cloud
    df_iter = pd.read_csv(
        file,
        sep=";",
        encoding="utf-8-sig",
        engine="python",
        chunksize=200_000
    )
    st.write("ğŸ§© csv iterator created")

    chunks_done = 0
    total_rows = 0
    progress = st.progress(0)

    with engine.begin() as conn:
        for chunk in df_iter:
            chunks_done += 1

            # Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸
            time_col = pick_col(chunk, ["Ğ’Ñ€ĞµĞ¼Ñ ĞºĞ»Ğ¸ĞºĞ°", "Ğ”Ğ°Ñ‚Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ", "Click time", "Click Time"])
            subid_col = pick_col(chunk, ["Subid", "SubId", "subid", "SUBID"])

            chunk["day"] = pd.to_datetime(chunk[time_col], errors="coerce").dt.date
            chunk["subid"] = chunk[subid_col].astype(str)

            agg = (
                chunk.dropna(subset=["day", "subid"])
                     .groupby(["day", "subid"])
                     .size()
                     .reset_index(name="clicks")
            )

            total_rows += len(chunk)
            st.write(f"â¬†ï¸ chunk #{chunks_done}: Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ» {total_rows:,} ÑÑ‚Ñ€Ğ¾Ğº, Ğ°Ğ³Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» {len(agg):,}â€¦")

            if agg.empty:
                continue

            # ---- DB pipeline: TRUNCATE staging -> COPY -> MERGE ----
            st.write("ğŸ§ª before TRUNCATE staging")
            conn.execute(text("truncate staging_clicks_daily;"))
            st.write("ğŸ§ª after TRUNCATE staging")

            st.write("ğŸ§ª before COPY to staging")
            copy_df_to_table(conn, agg[["day", "subid", "clicks"]], "staging_clicks_daily")
            st.write("ğŸ§ª after COPY to staging")

            st.write("ğŸ§ª before MERGE to fact")
            conn.execute(text("""
                insert into fact_clicks_daily(day, subid, clicks)
                select day, subid, clicks
                from staging_clicks_daily
                on conflict (day, subid)
                do update set clicks = fact_clicks_daily.clicks + excluded.clicks;
            """))
            st.write("ğŸ§ª after MERGE to fact")

            progress.progress(min(0.99, chunks_done / 20))

    progress.progress(1.0)
    st.write(f"ğŸ‰ clicks Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹, Ğ²ÑĞµĞ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº: {total_rows:,}")

def load_conversions(file):
    st.write("ğŸ§© start_load_conversions")

    df = read_csv_ru(file)

    subid_col = pick_col(df, ["Subid", "SubId", "subid", "SUBID"])
    status_col = pick_col(df, ["ĞÑ€Ğ¸Ğ³. ÑÑ‚Ğ°Ñ‚ÑƒÑ", "Orig. status", "Orig status", "Status"])
    conv_time_col = pick_col(df, ["Ğ’Ñ€ĞµĞ¼Ñ ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸", "Conversion time", "Time conversion"])
    sale_time_col = None
    # "Ğ’Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸" Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ â€” Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾
    for cand in ["Ğ’Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸", "Sale time", "Time sale"]:
        try:
            sale_time_col = pick_col(df, [cand])
            break
        except Exception:
            pass

    df["subid"] = df[subid_col].astype(str)
    df["_status"] = df[status_col].astype(str).str.lower()

    df["day_lead"] = pd.to_datetime(df[conv_time_col], errors="coerce").dt.date

    if sale_time_col:
        sale_time = df[sale_time_col].where(
            df[sale_time_col].notna() & (df[sale_time_col].astype(str) != ""),
            df[conv_time_col]
        )
    else:
        sale_time = df[conv_time_col]

    df["day_sale"] = pd.to_datetime(sale_time, errors="coerce").dt.date

    leads = (
        df[df["_status"] == "lead"]
        .dropna(subset=["day_lead", "subid"])
        .groupby(["day_lead", "subid"])
        .size()
        .reset_index(name="leads")
        .rename(columns={"day_lead": "day"})
    )

    sales = (
        df[df["_status"] == "sale"]
        .dropna(subset=["day_sale", "subid"])
        .groupby(["day_sale", "subid"])
        .size()
        .reset_index(name="sales")
        .rename(columns={"day_sale": "day"})
    )

    merged = (
        pd.merge(leads, sales, on=["day", "subid"], how="outer")
          .fillna(0)
          .astype({"leads": int, "sales": int})
    )

    st.write(f"ğŸ§ª conversions aggregated rows: {len(merged):,}")

        with engine.begin() as conn:
        st.write("ğŸ§ª before TRUNCATE conv staging")
        conn.execute(text("truncate staging_conversions_daily;"))
        st.write("ğŸ§ª after TRUNCATE conv staging")

        st.write("ğŸ§ª before COPY conv to staging")
        copy_df_to_table(conn, merged[["day", "subid", "leads", "sales"]], "staging_conversions_daily")
        st.write("ğŸ§ª after COPY conv to staging")

        st.write("ğŸ§ª before MERGE conv to fact")
        conn.execute(text("""
            insert into fact_conversions_daily(day, subid, leads, sales)
            select day, subid, leads, sales
            from staging_conversions_daily
            on conflict (day, subid)
            do update set
              leads = excluded.leads,
              sales = excluded.sales;
        """))
        st.write("ğŸ§ª after MERGE conv to fact")

    st.write("ğŸ‰ conversions Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")

# ---------------- UI ----------------
st.title("ğŸ“Š KT dashboard")
st.caption("build: 2025-12-23 v3 copy")

with st.sidebar:
    st.header("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° CSV")
    clicks = st.file_uploader("click.csv", type="csv")
    conv = st.file_uploader("conv.csv", type="csv")

    if st.button("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ² Ğ‘Ğ”", type="primary"):
        with st.spinner("Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ±Ğ°Ğ·Ñƒ..."):
            if clicks:
                st.write("ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ clicks...")
                load_clicks(clicks)
                st.write("âœ… clicks Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")

            if conv:
                st.write("ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ conversions...")
                load_conversions(conv)
                st.write("âœ… conversions Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")

        st.success("ğŸ‰ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ² Ğ‘Ğ”")

# ---------------- DASHBOARD ----------------
df = pd.read_sql("""
select
  c.day,
  c.subid,
  c.clicks,
  coalesce(v.leads,0) as leads,
  coalesce(v.sales,0) as sales
from fact_clicks_daily c
left join fact_conversions_daily v
  on v.day = c.day and v.subid = c.subid
order by c.day;
""", engine)

if df.empty:
    st.info("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸ CSV Ñ„Ğ°Ğ¹Ğ»Ñ‹ â€” Ğ¿Ğ¾ÑĞ²Ğ¸Ñ‚ÑÑ Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´.")
    st.stop()

last_day = df["day"].max()
prev_day = df[df["day"] < last_day]["day"].max()

k1, k2, k3 = st.columns(3)
k1.metric("ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸", int(df[df.day == last_day].sales.sum()))
k2.metric("Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸", int(df[df.day == last_day].leads.sum()))
k3.metric("ĞšĞ»Ğ¸ĞºĞ¸", int(df[df.day == last_day].clicks.sum()))

st.subheader("ğŸ“ˆ ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸ Ğ¿Ğ¾ Ğ´Ğ½ÑĞ¼")
st.line_chart(df.groupby("day")["sales"].sum())

if prev_day:
    st.subheader("ğŸš€ Ğ¢ĞĞŸ Subid Ğ¿Ğ¾ Ñ€Ğ¾ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶")

    today = df[df.day == last_day].groupby("subid")["sales"].sum()
    yday = df[df.day == prev_day].groupby("subid")["sales"].sum()

    growth = (
        today.subtract(yday, fill_value=0)
             .sort_values(ascending=False)
             .head(20)
             .reset_index(name="Î” sales")
    )

    st.dataframe(growth, use_container_width=True)
else:
    st.warning("ĞŸĞ¾ĞºĞ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ´ĞµĞ½ÑŒ Ğ² Ğ±Ğ°Ğ·Ğµ â€” Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ½ÑƒĞ¶ĞµĞ½ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 2 Ğ´Ğ½Ñ.")

import io
import datetime as dt

import pandas as pd
import streamlit as st
from sqlalchemy import create_engine, text

st.set_page_config(page_title="KT dashboard", layout="wide")


# ---------------- DB ----------------
@st.cache_resource
def get_engine():
    return create_engine(
        st.secrets["DATABASE_URL"],
        pool_pre_ping=True,
        future=True,
    )


engine = get_engine()


# ---------------- Helpers ----------------
def read_csv_ru(f):
    # utf-8-sig Ğ»ĞµÑ‡Ğ¸Ñ‚ BOM Ğ² Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ñ…
    return pd.read_csv(f, sep=";", encoding="utf-8-sig", engine="python")


def pick_col(df: pd.DataFrame, candidates: list[str]) -> str:
    stripped_map = {c.strip(): c for c in df.columns}
    for cand in candidates:
        if cand in stripped_map:
            return stripped_map[cand]

    lowered = {c.lower().strip(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower().strip()
        if key in lowered:
            return lowered[key]

    raise KeyError(
        f"ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ° {candidates}. "
        f"Ğ¤Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸: {list(df.columns)}"
    )


def copy_df_to_table(conn, df: pd.DataFrame, table: str):
    """
    COPY df -> table (Postgres) Ñ‡ĞµÑ€ĞµĞ· psycopg2 copy_expert.
    conn: SQLAlchemy Connection (Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ engine.begin()).
    """
    raw = conn.connection  # psycopg2 connection
    cur = raw.cursor()

    buf = io.StringIO()
    df.to_csv(buf, index=False, header=False)
    buf.seek(0)

    cols = ",".join(df.columns)
    sql = f"COPY {table} ({cols}) FROM STDIN WITH (FORMAT CSV)"

    cur.copy_expert(sql, buf)
    cur.close()

def fmt_pct_cell(val):
    # Ñ‚ĞµĞºÑÑ‚ Ğ² ÑÑ‡ĞµĞ¹ĞºĞµ
    if val is None or pd.isna(val):
        return "â€”"
    sign = "+" if val > 0 else ""
    return f"{sign}{val:.2f}%"

def style_pct_color(val):
    # Ñ†Ğ²ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ°
    if val is None or pd.isna(val):
        return ""
    if val > 0:
        return "color: #22c55e; font-weight: 700;"
    if val < 0:
        return "color: #ef4444; font-weight: 700;"
    return "color: #a3a3a3;"

def pct_change(curr: float, prev: float):
    if prev is None or prev == 0:
        return None
    return (curr - prev) / prev * 100.0


def metric_with_pct(label: str, curr: int, prev: int):
    d = pct_change(curr, prev)
    if d is None:
        st.metric(label, curr, delta="â€”", delta_color="off")
    else:
        st.metric(label, curr, delta=float(round(d, 2)), delta_color="normal")


# ---------------- Schema bootstrap (safe) ----------------
def ensure_schema():
    # Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ñƒ Ñ‚ĞµĞ±Ñ ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ½Ğ° Ğ²ÑÑĞºĞ¸Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹.
    with engine.begin() as conn:
        conn.execute(
            text(
                """
                create table if not exists fact_clicks_daily (
                  day date not null,
                  subid text not null,
                  clicks bigint not null,
                  primary key (day, subid)
                );
                """
            )
        )
        conn.execute(
            text(
                """
                create table if not exists fact_conversions_daily (
                  day date not null,
                  subid text not null,
                  leads bigint not null,
                  sales bigint not null,
                  primary key (day, subid)
                );
                """
            )
        )
        conn.execute(
            text(
                """
                create table if not exists staging_clicks_daily (
                  day date not null,
                  subid text not null,
                  clicks bigint not null
                );
                """
            )
        )
        conn.execute(
            text(
                """
                create table if not exists dim_subid (
                  subid text primary key,
                  offer text,
                  country_flag text,
                  os text,
                  sub_id_2 text,
                  campaign text,
                  sub_id_1 text,
                  updated_at timestamptz default now()
                );
                """
            )
        )
        conn.execute(text("create index if not exists idx_dim_subid_sub2 on dim_subid(sub_id_2);"))
        conn.execute(text("create index if not exists idx_dim_subid_campaign on dim_subid(campaign);"))
        conn.execute(text("create index if not exists idx_dim_subid_offer on dim_subid(offer);"))


ensure_schema()


# ---------------- Loaders ----------------
def load_clicks(file):
    st.write("ğŸ§© start_load_clicks")

    df_iter = pd.read_csv(
        file,
        sep=";",
        encoding="utf-8-sig",
        engine="python",
        chunksize=200_000,
    )
    st.write("ğŸ§© csv iterator created")

    chunks_done = 0
    total_rows = 0
    progress = st.progress(0)

    with engine.begin() as conn:
        for chunk in df_iter:
            chunks_done += 1

            time_col = pick_col(chunk, ["Ğ’Ñ€ĞµĞ¼Ñ ĞºĞ»Ğ¸ĞºĞ°", "Ğ”Ğ°Ñ‚Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ", "Click time", "Click Time"])
            subid_col = pick_col(chunk, ["Subid", "SubId", "subid", "SUBID"])

            # Ğ´Ğ¾Ğ¿. Ğ¿Ğ¾Ğ»Ñ Ğ¸Ğ· click.csv
            offer_col = pick_col(chunk, ["ĞÑ„Ñ„ĞµÑ€", "Offer"])
            flag_col = pick_col(chunk, ["Ğ¤Ğ»Ğ°Ğ³ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹", "Country flag", "Flag"])
            os_col = pick_col(chunk, ["ĞĞ¡", "OS"])
            sub2_col = pick_col(chunk, ["Sub ID 2", "Subid 2", "Sub2", "Sub ID2"])
            camp_col = pick_col(chunk, ["ĞšĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ", "Campaign"])
            sub1_col = pick_col(chunk, ["Sub ID 1", "Subid 1", "Sub1", "Sub ID1"])

            chunk["day"] = pd.to_datetime(chunk[time_col], errors="coerce").dt.date
            chunk["subid"] = chunk[subid_col].astype(str)

            # -------- DIM Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° (Ğ¿Ğ¾ subid) Ñ‡ĞµÑ€ĞµĞ· TEMP staging --------
            dim = chunk[[subid_col, offer_col, flag_col, os_col, sub2_col, camp_col, sub1_col]].copy()
            dim.columns = ["subid", "offer", "country_flag", "os", "sub_id_2", "campaign", "sub_id_1"]
            dim["subid"] = dim["subid"].astype(str)

            # Ğ²Ñ‹ĞºĞ¸Ğ´Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ subid
            dim = dim[dim["subid"].notna() & (dim["subid"].astype(str).str.len() > 0)]
            # Ğ¾Ğ´Ğ½Ğ° ÑÑ‚Ñ€Ğ¾ĞºĞ° Ğ½Ğ° subid Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ñ‡Ğ°Ğ½ĞºĞ° (Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ)
            dim = dim.drop_duplicates(subset=["subid"], keep="last")

            # TEMP table, Ğ±ĞµĞ· TRUNCATE/DELETE Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†
            conn.execute(text("drop table if exists staging_dim_subid_tmp;"))
            conn.execute(
                text(
                    """
                    create temporary table staging_dim_subid_tmp (
                      subid text,
                      offer text,
                      country_flag text,
                      os text,
                      sub_id_2 text,
                      campaign text,
                      sub_id_1 text
                    ) on commit drop;
                    """
                )
            )
            copy_df_to_table(conn, dim[["subid", "offer", "country_flag", "os", "sub_id_2", "campaign", "sub_id_1"]],
                             "staging_dim_subid_tmp")

            # UPDATE ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… (Ğ±ĞµÑ€Ñ‘Ğ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¿ÑƒÑÑ‚Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ· staging)
            conn.execute(
                text(
                    """
                    update dim_subid d
                    set
                      offer = coalesce(nullif(s.offer,''), d.offer),
                      country_flag = coalesce(nullif(s.country_flag,''), d.country_flag),
                      os = coalesce(nullif(s.os,''), d.os),
                      sub_id_2 = coalesce(nullif(s.sub_id_2,''), d.sub_id_2),
                      campaign = coalesce(nullif(s.campaign,''), d.campaign),
                      sub_id_1 = coalesce(nullif(s.sub_id_1,''), d.sub_id_1),
                      updated_at = now()
                    from staging_dim_subid_tmp s
                    where d.subid = s.subid;
                    """
                )
            )

            # INSERT Ğ½Ğ¾Ğ²Ñ‹Ñ…
            conn.execute(
                text(
                    """
                    insert into dim_subid(subid, offer, country_flag, os, sub_id_2, campaign, sub_id_1)
                    select s.subid, s.offer, s.country_flag, s.os, s.sub_id_2, s.campaign, s.sub_id_1
                    from staging_dim_subid_tmp s
                    left join dim_subid d on d.subid = s.subid
                    where d.subid is null;
                    """
                )
            )

            # -------- FACT clicks --------
            agg = (
                chunk.dropna(subset=["day", "subid"])
                .groupby(["day", "subid"])
                .size()
                .reset_index(name="clicks")
            )

            total_rows += len(chunk)
            st.write(f"â¬†ï¸ chunk #{chunks_done}: Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ» {total_rows:,} ÑÑ‚Ñ€Ğ¾Ğº, Ğ°Ğ³Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» {len(agg):,}â€¦")

            if agg.empty:
                continue

            # staging_clicks_daily (Ğ¿ĞµÑ€Ğ¼Ğ°Ğ½ĞµĞ½Ñ‚Ğ½Ğ°Ñ) â€” Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ TRUNCATE (Ñƒ Ñ‚ĞµĞ±Ñ direct connection)
            conn.execute(text("truncate staging_clicks_daily;"))
            copy_df_to_table(conn, agg[["day", "subid", "clicks"]], "staging_clicks_daily")

            conn.execute(
                text(
                    """
                    insert into fact_clicks_daily(day, subid, clicks)
                    select day, subid, clicks
                    from staging_clicks_daily
                    on conflict (day, subid)
                    do update set clicks = fact_clicks_daily.clicks + excluded.clicks;
                    """
                )
            )

            progress.progress(min(0.99, chunks_done / 20))

    progress.progress(1.0)
    st.write(f"ğŸ‰ clicks Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹, Ğ²ÑĞµĞ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº: {total_rows:,}")


def load_conversions(file):
    st.write("ğŸ§© start_load_conversions")

    df = read_csv_ru(file)

    subid_col = pick_col(df, ["Subid", "SubId", "subid", "SUBID"])
    status_col = pick_col(df, ["ĞÑ€Ğ¸Ğ³. ÑÑ‚Ğ°Ñ‚ÑƒÑ", "Orig. status", "Orig status", "Status"])
    conv_time_col = pick_col(df, ["Ğ’Ñ€ĞµĞ¼Ñ ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸", "Conversion time"])

    sale_time_col = None
    for cand in ["Ğ’Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸", "Sale time"]:
        try:
            sale_time_col = pick_col(df, [cand])
            break
        except Exception:
            pass

    df["subid"] = df[subid_col].astype(str)
    df["_status"] = df[status_col].astype(str).str.lower()

    df["day_lead"] = pd.to_datetime(df[conv_time_col], errors="coerce").dt.date

    if sale_time_col:
        sale_time = df[sale_time_col].where(
            df[sale_time_col].notna() & (df[sale_time_col].astype(str) != ""),
            df[conv_time_col],
        )
    else:
        sale_time = df[conv_time_col]

    df["day_sale"] = pd.to_datetime(sale_time, errors="coerce").dt.date

    leads = (
        df[df["_status"] == "lead"]
        .dropna(subset=["day_lead", "subid"])
        .groupby(["day_lead", "subid"])
        .size()
        .reset_index(name="leads")
        .rename(columns={"day_lead": "day"})
    )

    sales = (
        df[df["_status"] == "sale"]
        .dropna(subset=["day_sale", "subid"])
        .groupby(["day_sale", "subid"])
        .size()
        .reset_index(name="sales")
        .rename(columns={"day_sale": "day"})
    )

    merged = (
        pd.merge(leads, sales, on=["day", "subid"], how="outer")
        .fillna(0)
        .astype({"leads": int, "sales": int})
    )

    st.write(f"ğŸ§ª conversions aggregated rows: {len(merged):,}")

    with engine.begin() as conn:
        # TEMP staging (Ğ½Ğµ Ñ‚Ñ€Ğ¾Ğ³Ğ°ĞµĞ¼ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ truncate/delete)
        conn.execute(text("drop table if exists staging_conversions_tmp;"))
        conn.execute(
            text(
                """
                create temporary table staging_conversions_tmp (
                  day date,
                  subid text,
                  leads bigint,
                  sales bigint
                ) on commit drop;
                """
            )
        )

        copy_df_to_table(conn, merged[["day", "subid", "leads", "sales"]], "staging_conversions_tmp")

        # UPDATE ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ…
        conn.execute(
            text(
                """
                update fact_conversions_daily f
                set
                  leads = s.leads,
                  sales = s.sales
                from staging_conversions_tmp s
                where f.day = s.day and f.subid = s.subid;
                """
            )
        )

        # INSERT Ğ½Ğ¾Ğ²Ñ‹Ñ…
        conn.execute(
            text(
                """
                insert into fact_conversions_daily(day, subid, leads, sales)
                select s.day, s.subid, s.leads, s.sales
                from staging_conversions_tmp s
                left join fact_conversions_daily f
                  on f.day = s.day and f.subid = s.subid
                where f.subid is null;
                """
            )
        )

    st.write("ğŸ‰ conversions Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")


# ---------------- UI ----------------
st.title("ğŸ“Š KT dashboard")
st.caption("build: 2025-12-23 v4 dims + gainers")

with st.sidebar:
    st.header("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° CSV")
    clicks_file = st.file_uploader("click.csv", type="csv")
    conv_file = st.file_uploader("conv.csv", type="csv")

    if st.button("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ² Ğ‘Ğ”", type="primary"):
        with st.spinner("Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ±Ğ°Ğ·Ñƒ..."):
            if clicks_file:
                st.write("ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ clicks...")
                load_clicks(clicks_file)
                st.write("âœ… clicks Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")

            if conv_file:
                st.write("ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ conversions...")
                load_conversions(conv_file)
                st.write("âœ… conversions Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")

        st.success("ğŸ‰ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ² Ğ‘Ğ”")


# ---------------- Data for dashboard ----------------
df = pd.read_sql(
    """
    with keys as (
      select day, subid from fact_clicks_daily
      union
      select day, subid from fact_conversions_daily
    )
    select
      k.day,
      k.subid,
      coalesce(c.clicks, 0) as clicks,
      coalesce(v.leads, 0) as leads,
      coalesce(v.sales, 0) as sales,
      d.offer,
      d.country_flag,
      d.os,
      d.sub_id_2,
      d.campaign,
      d.sub_id_1
    from keys k
    left join fact_clicks_daily c
      on c.day = k.day and c.subid = k.subid
    left join fact_conversions_daily v
      on v.day = k.day and v.subid = k.subid
    left join dim_subid d
      on d.subid = k.subid
    order by k.day;
    """,
    engine,
)

if df.empty:
    st.info("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸ CSV Ñ„Ğ°Ğ¹Ğ»Ñ‹ â€” Ğ¿Ğ¾ÑĞ²Ğ¸Ñ‚ÑÑ Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´.")
    st.stop()

# ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ·Ğ¾Ğ²
df["sub_id_2"] = df["sub_id_2"].fillna("").astype(str).str.strip()
df["sub2_norm"] = df["sub_id_2"].replace({"": "Organic"})

df["campaign"] = df["campaign"].fillna("").astype(str)
df["campaign_short"] = df["campaign"].str.split("[", n=1).str[0].str.strip()

df["offer"] = df["offer"].fillna("").astype(str).str.strip()

# ĞŸĞµÑ€Ğ¸Ğ¾Ğ´Ñ‹: Ğ²Ñ‡ĞµÑ€Ğ° / Ğ¿Ğ¾Ğ·Ğ°Ğ²Ñ‡ĞµÑ€Ğ°
today = dt.date.today()
yday = today - dt.timedelta(days=1)
pday = today - dt.timedelta(days=2)

df_y = df[df["day"] == yday].copy()
df_p = df[df["day"] == pday].copy()

y_clicks = int(df_y["clicks"].sum())
p_clicks = int(df_p["clicks"].sum())

y_leads = int(df_y["leads"].sum())
p_leads = int(df_p["leads"].sum())

y_sales = int(df_y["sales"].sum())
p_sales = int(df_p["sales"].sum())

# KPI
k1, k2, k3 = st.columns(3)
with k1:
    metric_with_pct("Ğ˜Ğ½ÑÑ‚Ğ°Ğ»Ğ»Ñ‹", y_clicks, p_clicks)  # ĞºĞ°Ğº Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»: ĞºĞ»Ğ¸ĞºĞ¸ = Ğ¸Ğ½ÑÑ‚Ğ°Ğ»Ğ»Ñ‹
with k2:
    metric_with_pct("Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸", y_leads, p_leads)
with k3:
    metric_with_pct("ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸", y_sales, p_sales)

# Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶ Ğ¿Ğ¾ Ğ´Ğ½ÑĞ¼ (Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼)
st.subheader("ğŸ“ˆ ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸ Ğ¿Ğ¾ Ğ´Ğ½ÑĞ¼")
st.line_chart(df.groupby("day")["sales"].sum())

# --- Top 5 Sub ID 2 by Sales (exclude Organic) ---
st.subheader("ğŸ† Ğ¢Ğ¾Ğ¿ 5 Sub ID 2 Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ°Ğ¼ (Ğ²Ñ‡ĞµÑ€Ğ°)")

df_y_non_org = df_y[df_y["sub2_norm"] != "Organic"].copy()
df_p_non_org = df_p[df_p["sub2_norm"] != "Organic"].copy()

top_sub2_y = df_y_non_org.groupby("sub2_norm")["sales"].sum().sort_values(ascending=False).head(5)
top_sub2_p = df_p_non_org.groupby("sub2_norm")["sales"].sum()

rows = []
for sub2, s_y in top_sub2_y.items():
    s_p = float(top_sub2_p.get(sub2, 0))
    ch = pct_change(float(s_y), float(s_p))
    rows.append(
        {
            "Sub ID 2": sub2,
            "Sales (yday)": int(s_y),
            "Î”% vs prev": None if ch is None else round(ch, 2),
        }
    )
df_tbl = pd.DataFrame(rows)

# Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ°
df_tbl["Î”% vs prev"] = df_tbl["Î”% vs prev"].apply(lambda x: None if x is None else float(x))

sty = (
    df_tbl.style
    .format({"Î”% vs prev": fmt_pct_cell})
    .applymap(style_pct_color, subset=["Î”% vs prev"])
)

st.dataframe(sty, use_container_width=True)


# --- Top 5 Campaign by Sales (campaign_short) ---
st.subheader("ğŸ† Ğ¢Ğ¾Ğ¿ 5 ĞšĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ°Ğ¼ (Ğ²Ñ‡ĞµÑ€Ğ°)")

top_c_y = df_y.groupby("campaign_short")["sales"].sum().sort_values(ascending=False).head(5)
top_c_p = df_p.groupby("campaign_short")["sales"].sum()

rows = []
for camp, s_y in top_c_y.items():
    s_p = float(top_c_p.get(camp, 0))
    ch = pct_change(float(s_y), float(s_p))
    rows.append(
        {
            "Campaign": camp,
            "Sales (yday)": int(s_y),
            "Î”% vs prev": None if ch is None else round(ch, 2),
        }
    )
df_tbl = pd.DataFrame(rows)
df_tbl["Î”% vs prev"] = df_tbl["Î”% vs prev"].apply(lambda x: None if x is None else float(x))

sty = (
    df_tbl.style
    .format({"Î”% vs prev": fmt_pct_cell})
    .applymap(style_pct_color, subset=["Î”% vs prev"])
)

st.dataframe(sty, use_container_width=True)


# --- Gain helpers ---
def gain_table(group_col: str, metric_col: str, title: str, top_n: int = 10, exclude_organic: bool = False):
    st.subheader(title)

    a = df_y.copy()
    b = df_p.copy()

    if exclude_organic and group_col == "sub2_norm":
        a = a[a["sub2_norm"] != "Organic"]
        b = b[b["sub2_norm"] != "Organic"]

    y = a.groupby(group_col)[metric_col].sum()
    p = b.groupby(group_col)[metric_col].sum()

    out = (y.subtract(p, fill_value=0)).sort_values(ascending=False)
    out = out[out > 0].head(top_n)

    rows = []
    for key, delta in out.items():
        rows.append(
            {
                group_col: key,
                f"{metric_col} (yday)": int(y.get(key, 0)),
                f"{metric_col} (prev)": int(p.get(key, 0)),
                f"Î” {metric_col}": int(delta),
            }
        )

    st.dataframe(pd.DataFrame(rows), use_container_width=True)


# Traffic gainers (clicks)
gain_table("sub2_norm", "clicks", "ğŸ“ˆ Top 10 Sub ID 2 Traffic Gainers (ĞºĞ»Ğ¸ĞºĞ¸)", exclude_organic=True)
gain_table("campaign_short", "clicks", "ğŸ“ˆ Top 10 ĞšĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Traffic Gainers (ĞºĞ»Ğ¸ĞºĞ¸)")

# Sales gainers
gain_table("sub2_norm", "sales", "ğŸ’° Top 10 Sub ID 2 Sales Gainers (Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸)", exclude_organic=True)
gain_table("campaign_short", "sales", "ğŸ’° Top 10 ĞšĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Sales Gainers (Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸)")
gain_table("offer", "sales", "ğŸ’° Top 10 ĞÑ„Ñ„ĞµÑ€ Sales Gainers (Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸)")

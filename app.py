import pandas as pd
import streamlit as st
from sqlalchemy import create_engine, text

st.set_page_config(page_title="KT dashboard", layout="wide")

@st.cache_resource
def get_engine():
    return create_engine(
        st.secrets["DATABASE_URL"],
        pool_pre_ping=True
    )

engine = get_engine()

def read_csv_ru(f):
    return pd.read_csv(f, sep=";", encoding="utf-8-sig", engine="python")

# ---------- LOADERS ----------

def pick_col(df, candidates):
    cols = {c.strip(): c for c in df.columns}  # map stripped->original
    for cand in candidates:
        if cand in cols:
            return cols[cand]
    # fallback: try case-insensitive contains
    lowered = {c.lower().strip(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower().strip()
        if key in lowered:
            return lowered[key]
    raise KeyError(f"–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∏ –æ–¥–Ω–∞ –∏–∑ –∫–æ–ª–æ–Ω–æ–∫: {candidates}. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")

def load_clicks(file):
    st.write("üß© start load_clicks")

    df_iter = pd.read_csv(
        file,
        sep=";",
        encoding="utf-8-sig",
        engine="python",
        chunksize=150_000  # —á—É—Ç—å –º–µ–Ω—å—à–µ, —á—Ç–æ–±—ã —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
    )
    st.write("üß© csv iterator created")

    total_rows = 0
    chunks_done = 0
    progress = st.progress(0)

    def batched(records, batch_size=2000):
        for i in range(0, len(records), batch_size):
            yield records[i:i+batch_size]

    # –í–ê–ñ–ù–û: –≤—ã–ø–æ–ª–Ω—è–µ–º –º–Ω–æ–≥–æ –º–∞–ª–µ–Ω—å–∫–∏—Ö execute –≤–º–µ—Å—Ç–æ –æ–¥–Ω–æ–≥–æ –æ–≥—Ä–æ–º–Ω–æ–≥–æ
    upsert_sql = text("""
        insert into fact_clicks_daily(day, subid, clicks)
        values (:day, :subid, :clicks)
        on conflict (day, subid)
        do update set clicks = fact_clicks_daily.clicks + excluded.clicks
    """)

    with engine.begin() as conn:
        for chunk in df_iter:
            chunks_done += 1

            time_col = pick_col(chunk, ["–í—Ä–µ–º—è –∫–ª–∏–∫–∞", "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è"])
            subid_col = pick_col(chunk, ["Subid", "SubId", "subid", "SUBID"])

            chunk["day"] = pd.to_datetime(chunk[time_col], errors="coerce").dt.date
            chunk["subid"] = chunk[subid_col].astype(str)

            agg = (
                chunk.dropna(subset=["day", "subid"])
                     .groupby(["day", "subid"])
                     .size()
                     .reset_index(name="clicks")
            )

            total_rows += len(chunk)

            # ‚úÖ –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –î–û –≤—Å—Ç–∞–≤–∫–∏
            st.write(f"‚¨ÜÔ∏è chunk #{chunks_done}: –ø—Ä–æ—á–∏—Ç–∞–ª {total_rows:,} —Å—Ç—Ä–æ–∫, –≥–æ—Ç–æ–≤–ª—é {len(agg):,} upsert-—Å—Ç—Ä–æ–∫‚Ä¶")

            if agg.empty:
                continue

            records = agg.to_dict("records")

            # ‚úÖ –ø–∏—à–µ–º –ø–æ—Ä—Ü–∏—è–º–∏
            batches = 0
            for b in batched(records, batch_size=2000):
                conn.execute(upsert_sql, b)
                batches += 1

            st.write(f"‚úÖ chunk #{chunks_done}: –∑–∞–ø–∏—Å–∞–Ω–æ {len(records):,} —Å—Ç—Ä–æ–∫ –≤ {batches} –±–∞—Ç—á–∞—Ö")

            # –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä ‚Äú–ø–æ –æ—â—É—â–µ–Ω–∏—è–º‚Äù (–ø–æ—Å–∫–æ–ª—å–∫—É —Ç–æ—á–Ω–æ–≥–æ total —Å—Ç—Ä–æ–∫ –º—ã –Ω–µ –∑–Ω–∞–µ–º –∑–∞—Ä–∞–Ω–µ–µ)
            progress.progress(min(0.99, chunks_done / 20))

    progress.progress(1.0)
    st.write(f"üéâ clicks –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –≤—Å–µ–≥–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {total_rows:,}")

def load_conversions(file):
    df = read_csv_ru(file)
    df["subid"] = df["Subid"]

    df["day_lead"] = pd.to_datetime(df["–í—Ä–µ–º—è –∫–æ–Ω–≤–µ—Ä—Å–∏–∏"], errors="coerce").dt.date

    sale_time = df["–í—Ä–µ–º—è –ø—Ä–æ–¥–∞–∂–∏"].where(
        df["–í—Ä–µ–º—è –ø—Ä–æ–¥–∞–∂–∏"].notna() & (df["–í—Ä–µ–º—è –ø—Ä–æ–¥–∞–∂–∏"] != ""),
        df["–í—Ä–µ–º—è –∫–æ–Ω–≤–µ—Ä—Å–∏–∏"]
    )
    df["day_sale"] = pd.to_datetime(sale_time, errors="coerce").dt.date

    leads = (
        df[df["–û—Ä–∏–≥. —Å—Ç–∞—Ç—É—Å"].str.lower() == "lead"]
        .dropna(subset=["day_lead", "subid"])
        .groupby(["day_lead", "subid"])
        .size()
        .reset_index(name="leads")
        .rename(columns={"day_lead": "day"})
    )

    sales = (
        df[df["–û—Ä–∏–≥. —Å—Ç–∞—Ç—É—Å"].str.lower() == "sale"]
        .dropna(subset=["day_sale", "subid"])
        .groupby(["day_sale", "subid"])
        .size()
        .reset_index(name="sales")
        .rename(columns={"day_sale": "day"})
    )

    merged = (
        pd.merge(leads, sales, on=["day", "subid"], how="outer")
          .fillna(0)
          .astype({"leads": int, "sales": int})
    )

    with engine.begin() as conn:
        conn.execute(
            text("""
            insert into fact_conversions_daily(day, subid, leads, sales)
            values (:day, :subid, :leads, :sales)
            on conflict (day, subid)
            do update set
              leads = excluded.leads,
              sales = excluded.sales
            """),
            merged.to_dict("records")
        )

# ---------- UI ----------

st.title("üìä KT dashboard")
st.caption("build: 2025-12-23 v2 chunks")

with st.sidebar:
    st.header("–ó–∞–≥—Ä—É–∑–∫–∞ CSV")
    clicks = st.file_uploader("click.csv", type="csv")
    conv = st.file_uploader("conv.csv", type="csv")

if st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ –ë–î", type="primary"):
    with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –≤ –±–∞–∑—É..."):
        if clicks:
            st.write("üì• –ó–∞–≥—Ä—É–∂–∞—é clicks...")
            load_clicks(clicks)
            st.write("‚úÖ clicks –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

        if conv:
            st.write("üì• –ó–∞–≥—Ä—É–∂–∞—é conversions...")
            load_conversions(conv)
            st.write("‚úÖ conversions –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

    st.success("üéâ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ë–î")

# ---------- DASHBOARD ----------

df = pd.read_sql("""
select
  c.day,
  c.subid,
  c.clicks,
  coalesce(v.leads,0) as leads,
  coalesce(v.sales,0) as sales
from fact_clicks_daily c
left join fact_conversions_daily v
  on v.day = c.day and v.subid = c.subid
order by c.day;
""", engine)

if df.empty:
    st.info("–ó–∞–≥—Ä—É–∑–∏ CSV —Ñ–∞–π–ª—ã")
    st.stop()

last_day = df["day"].max()
prev_day = df[df["day"] < last_day]["day"].max()

k1, k2, k3 = st.columns(3)
k1.metric("–ü—Ä–æ–¥–∞–∂–∏", int(df[df.day == last_day].sales.sum()))
k2.metric("–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏", int(df[df.day == last_day].leads.sum()))
k3.metric("–ö–ª–∏–∫–∏", int(df[df.day == last_day].clicks.sum()))

st.subheader("üìà –ü—Ä–æ–¥–∞–∂–∏ –ø–æ –¥–Ω—è–º")
st.line_chart(df.groupby("day")["sales"].sum())

if prev_day:
    st.subheader("üöÄ –¢–û–ü Subid –ø–æ —Ä–æ—Å—Ç—É –ø—Ä–æ–¥–∞–∂")

    today = df[df.day == last_day].groupby("subid")["sales"].sum()
    yday = df[df.day == prev_day].groupby("subid")["sales"].sum()

    growth = (
        today.subtract(yday, fill_value=0)
             .sort_values(ascending=False)
             .head(20)
             .reset_index(name="Œî sales")
    )

    st.dataframe(growth, use_container_width=True)

import io
import pandas as pd
import streamlit as st
from sqlalchemy import create_engine, text

st.set_page_config(page_title="KT dashboard", layout="wide")

# ---------------- DB ----------------
@st.cache_resource
def get_engine():
    return create_engine(
        st.secrets["DATABASE_URL"],
        pool_pre_ping=True,
        future=True
    )

engine = get_engine()

# ---------------- Helpers ----------------
def read_csv_ru(f):
    # utf-8-sig –ª–µ—á–∏—Ç BOM –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
    return pd.read_csv(f, sep=";", encoding="utf-8-sig", engine="python")

def pick_col(df: pd.DataFrame, candidates: list[str]) -> str:
    # 1) —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ strip
    stripped_map = {c.strip(): c for c in df.columns}
    for cand in candidates:
        if cand in stripped_map:
            return stripped_map[cand]

    # 2) case-insensitive —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    lowered = {c.lower().strip(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower().strip()
        if key in lowered:
            return lowered[key]

    raise KeyError(
        f"–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ {candidates}. "
        f"–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}"
    )

def copy_df_to_table(conn, df: pd.DataFrame, table: str):
    """
    COPY df -> table (Postgres) —á–µ—Ä–µ–∑ psycopg2 copy_expert.
    conn: SQLAlchemy Connection (–≤–Ω—É—Ç—Ä–∏ engine.begin()).
    """
    raw = conn.connection  # psycopg2 connection
    cur = raw.cursor()

    buf = io.StringIO()
    # –ë–µ–∑ header, –∏–Ω–∞—á–µ COPY –±—É–¥–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –≤—Å—Ç–∞–≤–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ –¥–∞–Ω–Ω—ã–µ
    df.to_csv(buf, index=False, header=False)
    buf.seek(0)

    cols = ",".join(df.columns)
    sql = f"COPY {table} ({cols}) FROM STDIN WITH (FORMAT CSV)"

    cur.copy_expert(sql, buf)
    cur.close()

# ---------------- Loaders ----------------
def load_clicks(file):
    st.write("üß© start_load_clicks")

    # –ß–∏—Ç–∞–µ–º –∫—É—Å–∫–∞–º–∏ ‚Äî —á—Ç–æ–±—ã –Ω–µ —É–±–∏–≤–∞—Ç—å –ø–∞–º—è—Ç—å –Ω–∞ Streamlit Cloud
    df_iter = pd.read_csv(
        file,
        sep=";",
        encoding="utf-8-sig",
        engine="python",
        chunksize=200_000
    )
    st.write("üß© csv iterator created")

    chunks_done = 0
    total_rows = 0
    progress = st.progress(0)

    with engine.begin() as conn:
        for chunk in df_iter:
            chunks_done += 1

            # –≤—ã–±–∏—Ä–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏
            time_col = pick_col(chunk, ["–í—Ä–µ–º—è –∫–ª–∏–∫–∞", "–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è", "Click time", "Click Time"])
            subid_col = pick_col(chunk, ["Subid", "SubId", "subid", "SUBID"])

            chunk["day"] = pd.to_datetime(chunk[time_col], errors="coerce").dt.date
            chunk["subid"] = chunk[subid_col].astype(str)

            agg = (
                chunk.dropna(subset=["day", "subid"])
                     .groupby(["day", "subid"])
                     .size()
                     .reset_index(name="clicks")
            )

            total_rows += len(chunk)
            st.write(f"‚¨ÜÔ∏è chunk #{chunks_done}: –ø—Ä–æ—á–∏—Ç–∞–ª {total_rows:,} —Å—Ç—Ä–æ–∫, –∞–≥–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–ª {len(agg):,}‚Ä¶")

            if agg.empty:
                continue

            # ---- DB pipeline: TRUNCATE staging -> COPY -> MERGE ----
            st.write("üß™ before TRUNCATE staging")
            conn.execute(text("truncate staging_clicks_daily;"))
            st.write("üß™ after TRUNCATE staging")

            st.write("üß™ before COPY to staging")
            copy_df_to_table(conn, agg[["day", "subid", "clicks"]], "staging_clicks_daily")
            st.write("üß™ after COPY to staging")

            st.write("üß™ before MERGE to fact")
            conn.execute(text("""
                insert into fact_clicks_daily(day, subid, clicks)
                select day, subid, clicks
                from staging_clicks_daily
                on conflict (day, subid)
                do update set clicks = fact_clicks_daily.clicks + excluded.clicks;
            """))
            st.write("üß™ after MERGE to fact")

            progress.progress(min(0.99, chunks_done / 20))

    progress.progress(1.0)
    st.write(f"üéâ clicks –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –≤—Å–µ–≥–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {total_rows:,}")

def load_conversions(file):
    st.write("üß© start_load_conversions")

    df = read_csv_ru(file)

    subid_col = pick_col(df, ["Subid", "SubId", "subid", "SUBID"])
    status_col = pick_col(df, ["–û—Ä–∏–≥. —Å—Ç–∞—Ç—É—Å", "Orig. status", "Orig status", "Status"])
    conv_time_col = pick_col(df, ["–í—Ä–µ–º—è –∫–æ–Ω–≤–µ—Ä—Å–∏–∏", "Conversion time"])

    sale_time_col = None
    for cand in ["–í—Ä–µ–º—è –ø—Ä–æ–¥–∞–∂–∏", "Sale time"]:
        try:
            sale_time_col = pick_col(df, [cand])
            break
        except Exception:
            pass

    df["subid"] = df[subid_col].astype(str)
    df["_status"] = df[status_col].astype(str).str.lower()

    df["day_lead"] = pd.to_datetime(df[conv_time_col], errors="coerce").dt.date

    if sale_time_col:
        sale_time = df[sale_time_col].where(
            df[sale_time_col].notna() & (df[sale_time_col].astype(str) != ""),
            df[conv_time_col]
        )
    else:
        sale_time = df[conv_time_col]

    df["day_sale"] = pd.to_datetime(sale_time, errors="coerce").dt.date

    leads = (
        df[df["_status"] == "lead"]
        .dropna(subset=["day_lead", "subid"])
        .groupby(["day_lead", "subid"])
        .size()
        .reset_index(name="leads")
        .rename(columns={"day_lead": "day"})
    )

    sales = (
        df[df["_status"] == "sale"]
        .dropna(subset=["day_sale", "subid"])
        .groupby(["day_sale", "subid"])
        .size()
        .reset_index(name="sales")
        .rename(columns={"day_sale": "day"})
    )

    merged = (
        pd.merge(leads, sales, on=["day", "subid"], how="outer")
        .fillna(0)
        .astype({"leads": int, "sales": int})
    )

    st.write(f"üß™ conversions aggregated rows: {len(merged):,}")

    # üîΩ –í–ê–ñ–ù–û: —ç—Ç–æ—Ç with –ù–ê –¢–û–ú –ñ–ï –£–†–û–í–ù–ï, —á—Ç–æ –∏ merged
    with engine.begin() as conn:
        st.write("üß™ before DELETE conv staging")
        conn.execute(text("delete from staging_conversions_daily;"))
        st.write("üß™ after DELETE conv staging")

        st.write("üß™ before COPY conv to staging")
        copy_df_to_table(
            conn,
            merged[["day", "subid", "leads", "sales"]],
            "staging_conversions_daily"
        )
        st.write("üß™ after COPY conv to staging")

        # üîç –õ–û–ì ‚Ññ1 ‚Äî –ø–µ—Ä–µ–¥ UPDATE
        st.write("üß™ before UPDATE fact")
        
        # 1) UPDATE —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫
        conn.execute(text("""
            update fact_conversions_daily f
            set
              leads = s.leads,
              sales = s.sales
            from staging_conversions_daily s
            where f.day = s.day and f.subid = s.subid;
        """))
        
        # üîç –õ–û–ì ‚Ññ2 ‚Äî –ø–æ—Å–ª–µ UPDATE, –ø–µ—Ä–µ–¥ INSERT
        st.write("üß™ after UPDATE fact, before INSERT new")
        
        # 2) INSERT —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫
        conn.execute(text("""
            insert into fact_conversions_daily(day, subid, leads, sales)
            select s.day, s.subid, s.leads, s.sales
            from staging_conversions_daily s
            left join fact_conversions_daily f
              on f.day = s.day and f.subid = s.subid
            where f.subid is null;
        """))
        
        # üîç –õ–û–ì ‚Ññ3 ‚Äî –ø–æ—Å–ª–µ INSERT
        st.write("üß™ after INSERT new")


        st.write("üß™ after MERGE conv to fact")

    st.write("üéâ conversions –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

# ---------------- UI ----------------
st.title("üìä KT dashboard")
st.caption("build: 2025-12-23 v3 copy")

with st.sidebar:
    st.header("–ó–∞–≥—Ä—É–∑–∫–∞ CSV")
    clicks = st.file_uploader("click.csv", type="csv")
    conv = st.file_uploader("conv.csv", type="csv")

    if st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ –ë–î", type="primary"):
        with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –≤ –±–∞–∑—É..."):
            if clicks:
                st.write("üì• –ó–∞–≥—Ä—É–∂–∞—é clicks...")
                load_clicks(clicks)
                st.write("‚úÖ clicks –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

            if conv:
                st.write("üì• –ó–∞–≥—Ä—É–∂–∞—é conversions...")
                load_conversions(conv)
                st.write("‚úÖ conversions –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

        st.success("üéâ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ë–î")

# ---------------- DASHBOARD ----------------
df = pd.read_sql("""
with keys as (
    select day, subid from fact_clicks_daily
    union
    select day, subid from fact_conversions_daily
)
select
    k.day,
    k.subid,
    coalesce(c.clicks, 0) as clicks,
    coalesce(v.leads, 0) as leads,
    coalesce(v.sales, 0) as sales
from keys k
left join fact_clicks_daily c
  on c.day = k.day and c.subid = k.subid
left join fact_conversions_daily v
  on v.day = k.day and v.subid = k.subid
order by k.day;
""", engine)

if df.empty:
    st.info("–ó–∞–≥—Ä—É–∑–∏ CSV —Ñ–∞–π–ª—ã ‚Äî –ø–æ—è–≤–∏—Ç—Å—è –¥–∞—à–±–æ—Ä–¥.")
    st.stop()

import datetime as dt

today = dt.date.today()
last_day = today - dt.timedelta(days=1)  # –≤—á–µ—Ä–∞
prev_day = last_day - dt.timedelta(days=1)  # –ø–æ–∑–∞–≤—á–µ—Ä–∞

st.write("–î–Ω–∏ –≤ –±–∞–∑–µ:", sorted(df["day"].unique()))

k1, k2, k3 = st.columns(3)
k1.metric("–ü—Ä–æ–¥–∞–∂–∏", int(df[df.day == last_day].sales.sum()))
k2.metric("–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏", int(df[df.day == last_day].leads.sum()))
k3.metric("–ö–ª–∏–∫–∏", int(df[df.day == last_day].clicks.sum()))

st.subheader("üìà –ü—Ä–æ–¥–∞–∂–∏ –ø–æ –¥–Ω—è–º")
st.line_chart(df.groupby("day")["sales"].sum())

if prev_day:
    st.subheader("üöÄ –¢–û–ü Subid –ø–æ —Ä–æ—Å—Ç—É –ø—Ä–æ–¥–∞–∂")

    today = df[df.day == last_day].groupby("subid")["sales"].sum()
    yday = df[df.day == prev_day].groupby("subid")["sales"].sum()

    growth = (
        today.subtract(yday, fill_value=0)
             .sort_values(ascending=False)
             .head(20)
             .reset_index(name="Œî sales")
    )

    st.dataframe(growth, use_container_width=True)
else:
    st.warning("–ü–æ–∫–∞ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –¥–µ–Ω—å –≤ –±–∞–∑–µ ‚Äî –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω—É–∂–µ–Ω –º–∏–Ω–∏–º—É–º 2 –¥–Ω—è.")

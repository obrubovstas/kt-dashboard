import io
import datetime as dt

import pandas as pd
import streamlit as st
from sqlalchemy import create_engine, text

st.set_page_config(page_title="KT dashboard", layout="wide")


# ===================== DB =====================
@st.cache_resource
def get_engine():
    return create_engine(
        st.secrets["DATABASE_URL"],
        pool_pre_ping=True,
        future=True,
    )


engine = get_engine()


# ===================== Helpers =====================
def pick_col(df: pd.DataFrame, candidates: list[str]) -> str:
    # 1) exact (strip)
    stripped_map = {c.strip(): c for c in df.columns}
    for cand in candidates:
        if cand in stripped_map:
            return stripped_map[cand]

    # 2) case-insensitive
    lowered = {c.lower().strip(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower().strip()
        if key in lowered:
            return lowered[key]

    raise KeyError(
        f"ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ° {candidates}. "
        f"Ğ¤Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸: {list(df.columns)}"
    )


def read_csv_ru(f):
    return pd.read_csv(f, sep=";", encoding="utf-8-sig", engine="python")


def copy_df_to_table(conn, df: pd.DataFrame, table: str):
    """
    COPY df -> table (Postgres) Ñ‡ĞµÑ€ĞµĞ· psycopg2 copy_expert.
    conn: SQLAlchemy Connection (Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ engine.begin()).
    """
    raw = conn.connection  # psycopg2 connection
    cur = raw.cursor()

    buf = io.StringIO()
    df.to_csv(buf, index=False, header=False)
    buf.seek(0)

    cols = ",".join(df.columns)
    sql = f"COPY {table} ({cols}) FROM STDIN WITH (FORMAT CSV)"

    cur.copy_expert(sql, buf)
    cur.close()


def pct_change(curr: float, prev: float):
    if prev is None or prev == 0:
        return None
    return (curr - prev) / prev * 100.0


def metric_with_pct(label: str, curr: int, prev: int):
    d = pct_change(curr, prev)
    if d is None:
        st.metric(label, curr, delta="â€”", delta_color="off")
    else:
        st.metric(label, curr, delta=float(round(d, 2)), delta_color="normal")


def fmt_pct_cell(val):
    if val is None or pd.isna(val):
        return "â€”"
    sign = "+" if val > 0 else ""
    return f"{sign}{val:.2f}%"


def style_pct_color(val):
    if val is None or pd.isna(val):
        return ""
    if val > 0:
        return "color: #22c55e; font-weight: 700;"
    if val < 0:
        return "color: #ef4444; font-weight: 700;"
    return "color: #a3a3a3;"


# ===================== Schema bootstrap =====================
def ensure_schema():
    with engine.begin() as conn:
        conn.execute(
            text(
                """
                create table if not exists fact_clicks_daily (
                  day date not null,
                  subid text not null,
                  clicks bigint not null,
                  primary key (day, subid)
                );
                """
            )
        )
        conn.execute(
            text(
                """
                create table if not exists fact_conversions_daily (
                  day date not null,
                  subid text not null,
                  leads bigint not null,
                  sales bigint not null,
                  primary key (day, subid)
                );
                """
            )
        )
        conn.execute(
            text(
                """
                create table if not exists staging_clicks_daily (
                  day date not null,
                  subid text not null,
                  clicks bigint not null
                );
                """
            )
        )
        conn.execute(
            text(
                """
                create table if not exists dim_subid (
                  subid text primary key,
                  offer text,
                  country_flag text,
                  os text,
                  sub_id_2 text,
                  campaign text,
                  sub_id_1 text,
                  updated_at timestamptz default now()
                );
                """
            )
        )
        conn.execute(text("create index if not exists idx_dim_subid_sub2 on dim_subid(sub_id_2);"))
        conn.execute(text("create index if not exists idx_dim_subid_campaign on dim_subid(campaign);"))
        conn.execute(text("create index if not exists idx_dim_subid_offer on dim_subid(offer);"))


ensure_schema()


# ===================== Loaders =====================
def load_clicks(file, mode: str = "replace_day"):
    """
    mode:
      - replace_day: Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ (idempotent) â€” Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ´Ğ½Ñ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ clicks Ğ·Ğ° ÑÑ‚Ğ¾Ñ‚ day
      - append: Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ (ĞµÑĞ»Ğ¸ Ğ³Ñ€ÑƒĞ·Ğ¸ÑˆÑŒ ĞºÑƒÑĞºĞ°Ğ¼Ğ¸/Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ´ĞµĞ½ÑŒ)
    """
    st.write("ğŸ§© start_load_clicks")

    df_iter = pd.read_csv(
        file,
        sep=";",
        encoding="utf-8-sig",
        engine="python",
        chunksize=200_000,
    )
    st.write("ğŸ§© csv iterator created")

    chunks_done = 0
    total_rows = 0
    progress = st.progress(0)

    detected_day = None

    with engine.begin() as conn:
        for chunk in df_iter:
            chunks_done += 1

            time_col = pick_col(chunk, ["Ğ’Ñ€ĞµĞ¼Ñ ĞºĞ»Ğ¸ĞºĞ°", "Ğ”Ğ°Ñ‚Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ", "Click time", "Click Time"])
            subid_col = pick_col(chunk, ["Subid", "SubId", "subid", "SUBID"])

            # Ğ´Ğ¾Ğ¿. Ğ¿Ğ¾Ğ»Ñ Ğ¸Ğ· click.csv
            offer_col = pick_col(chunk, ["ĞÑ„Ñ„ĞµÑ€", "Offer"])
            flag_col = pick_col(chunk, ["Ğ¤Ğ»Ğ°Ğ³ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹", "Country flag", "Flag"])
            os_col = pick_col(chunk, ["ĞĞ¡", "OS"])
            sub2_col = pick_col(chunk, ["Sub ID 2", "Subid 2", "Sub2", "Sub ID2"])
            camp_col = pick_col(chunk, ["ĞšĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ", "Campaign"])
            sub1_col = pick_col(chunk, ["Sub ID 1", "Subid 1", "Sub1", "Sub ID1"])

            chunk["day"] = pd.to_datetime(chunk[time_col], errors="coerce").dt.date
            chunk["subid"] = chunk[subid_col].astype(str)

            # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ´ĞµĞ½ÑŒ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ Ñ‡Ğ°Ğ½ĞºĞµ
            if detected_day is None:
                detected_day = chunk["day"].dropna().min()
                if detected_day is not None and mode == "replace_day":
                    st.write(f"ğŸ§¹ replace_day: Ğ¾Ñ‡Ğ¸Ñ‰Ğ°Ñ clicks Ğ·Ğ° {detected_day}")
                    conn.execute(text("delete from fact_clicks_daily where day = :d"), {"d": detected_day})

            # -------- DIM (subid -> attrs) Ñ‡ĞµÑ€ĞµĞ· TEMP staging --------
            dim = chunk[[subid_col, offer_col, flag_col, os_col, sub2_col, camp_col, sub1_col]].copy()
            dim.columns = ["subid", "offer", "country_flag", "os", "sub_id_2", "campaign", "sub_id_1"]
            dim["subid"] = dim["subid"].astype(str)

            dim = dim[dim["subid"].notna() & (dim["subid"].astype(str).str.len() > 0)]
            dim = dim.drop_duplicates(subset=["subid"], keep="last")

            conn.execute(text("drop table if exists staging_dim_subid_tmp;"))
            conn.execute(
                text(
                    """
                    create temporary table staging_dim_subid_tmp (
                      subid text,
                      offer text,
                      country_flag text,
                      os text,
                      sub_id_2 text,
                      campaign text,
                      sub_id_1 text
                    ) on commit drop;
                    """
                )
            )
            if not dim.empty:
                copy_df_to_table(
                    conn,
                    dim[["subid", "offer", "country_flag", "os", "sub_id_2", "campaign", "sub_id_1"]],
                    "staging_dim_subid_tmp",
                )

                conn.execute(
                    text(
                        """
                        update dim_subid d
                        set
                          offer = coalesce(nullif(s.offer,''), d.offer),
                          country_flag = coalesce(nullif(s.country_flag,''), d.country_flag),
                          os = coalesce(nullif(s.os,''), d.os),
                          sub_id_2 = coalesce(nullif(s.sub_id_2,''), d.sub_id_2),
                          campaign = coalesce(nullif(s.campaign,''), d.campaign),
                          sub_id_1 = coalesce(nullif(s.sub_id_1,''), d.sub_id_1),
                          updated_at = now()
                        from staging_dim_subid_tmp s
                        where d.subid = s.subid;
                        """
                    )
                )

                conn.execute(
                    text(
                        """
                        insert into dim_subid(subid, offer, country_flag, os, sub_id_2, campaign, sub_id_1)
                        select s.subid, s.offer, s.country_flag, s.os, s.sub_id_2, s.campaign, s.sub_id_1
                        from staging_dim_subid_tmp s
                        left join dim_subid d on d.subid = s.subid
                        where d.subid is null;
                        """
                    )
                )

            # -------- FACT clicks --------
            # ĞµÑĞ»Ğ¸ mode=replace_day, Ğ²ÑÑ‘ Ñ€Ğ°Ğ²Ğ½Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾: Ğ¼Ñ‹ ÑƒĞ¶Ğµ ÑƒĞ´Ğ°Ğ»Ğ¸Ğ»Ğ¸ day Ğ¸ Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾ Ğ·Ğ°Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼
            agg = (
                chunk.dropna(subset=["day", "subid"])
                .groupby(["day", "subid"])
                .size()
                .reset_index(name="clicks")
            )

            total_rows += len(chunk)
            st.write(f"â¬†ï¸ chunk #{chunks_done}: Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ» {total_rows:,} ÑÑ‚Ñ€Ğ¾Ğº, Ğ°Ğ³Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» {len(agg):,}â€¦")

            if agg.empty:
                continue

            conn.execute(text("truncate staging_clicks_daily;"))
            copy_df_to_table(conn, agg[["day", "subid", "clicks"]], "staging_clicks_daily")

            if mode == "append":
                # additive
                conn.execute(
                    text(
                        """
                        insert into fact_clicks_daily(day, subid, clicks)
                        select day, subid, clicks
                        from staging_clicks_daily
                        on conflict (day, subid)
                        do update set clicks = fact_clicks_daily.clicks + excluded.clicks;
                        """
                    )
                )
            else:
                # replace (idempotent) â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ
                conn.execute(
                    text(
                        """
                        insert into fact_clicks_daily(day, subid, clicks)
                        select day, subid, clicks
                        from staging_clicks_daily
                        on conflict (day, subid)
                        do update set clicks = excluded.clicks;
                        """
                    )
                )

            progress.progress(min(0.99, chunks_done / 20))

    progress.progress(1.0)
    st.write(f"ğŸ‰ clicks Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹, Ğ²ÑĞµĞ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº: {total_rows:,}")


def load_conversions(file):
    st.write("ğŸ§© start_load_conversions")

    df = read_csv_ru(file)

    subid_col = pick_col(df, ["Subid", "SubId", "subid", "SUBID"])
    status_col = pick_col(df, ["ĞÑ€Ğ¸Ğ³. ÑÑ‚Ğ°Ñ‚ÑƒÑ", "Orig. status", "Orig status", "Status"])
    conv_time_col = pick_col(df, ["Ğ’Ñ€ĞµĞ¼Ñ ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸", "Conversion time"])

    sale_time_col = None
    for cand in ["Ğ’Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸", "Sale time"]:
        try:
            sale_time_col = pick_col(df, [cand])
            break
        except Exception:
            pass

    df["subid"] = df[subid_col].astype(str)
    df["_status"] = df[status_col].astype(str).str.lower()

    df["day_lead"] = pd.to_datetime(df[conv_time_col], errors="coerce").dt.date

    if sale_time_col:
        sale_time = df[sale_time_col].where(
            df[sale_time_col].notna() & (df[sale_time_col].astype(str) != ""),
            df[conv_time_col],
        )
    else:
        sale_time = df[conv_time_col]

    df["day_sale"] = pd.to_datetime(sale_time, errors="coerce").dt.date

    leads = (
        df[df["_status"] == "lead"]
        .dropna(subset=["day_lead", "subid"])
        .groupby(["day_lead", "subid"])
        .size()
        .reset_index(name="leads")
        .rename(columns={"day_lead": "day"})
    )

    sales = (
        df[df["_status"] == "sale"]
        .dropna(subset=["day_sale", "subid"])
        .groupby(["day_sale", "subid"])
        .size()
        .reset_index(name="sales")
        .rename(columns={"day_sale": "day"})
    )

    merged = (
        pd.merge(leads, sales, on=["day", "subid"], how="outer")
        .fillna(0)
        .astype({"leads": int, "sales": int})
    )

    st.write(f"ğŸ§ª conversions aggregated rows: {len(merged):,}")

    with engine.begin() as conn:
        conn.execute(text("drop table if exists staging_conversions_tmp;"))
        conn.execute(
            text(
                """
                create temporary table staging_conversions_tmp (
                  day date,
                  subid text,
                  leads bigint,
                  sales bigint
                ) on commit drop;
                """
            )
        )

        if not merged.empty:
            copy_df_to_table(conn, merged[["day", "subid", "leads", "sales"]], "staging_conversions_tmp")

        # UPDATE ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ…
        conn.execute(
            text(
                """
                update fact_conversions_daily f
                set
                  leads = s.leads,
                  sales = s.sales
                from staging_conversions_tmp s
                where f.day = s.day and f.subid = s.subid;
                """
            )
        )

        # INSERT Ğ½Ğ¾Ğ²Ñ‹Ñ…
        conn.execute(
            text(
                """
                insert into fact_conversions_daily(day, subid, leads, sales)
                select s.day, s.subid, s.leads, s.sales
                from staging_conversions_tmp s
                left join fact_conversions_daily f
                  on f.day = s.day and f.subid = s.subid
                where f.subid is null;
                """
            )
        )

    st.write("ğŸ‰ conversions Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")


# ===================== UI =====================
st.title("ğŸ“Š KT dashboard")
st.caption("build: 2025-12-23 v4.1 (% gainers + NEW + dim_subid)")

with st.sidebar:
    st.header("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° CSV")
    clicks_file = st.file_uploader("click.csv", type="csv")
    conv_file = st.file_uploader("conv.csv", type="csv")

    load_mode = st.radio(
        "Ğ ĞµĞ¶Ğ¸Ğ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ clicks",
        options=["replace_day", "append"],
        index=0,
        help="replace_day â€” Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ‚ÑŒ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸ Ñ‚Ğ¾Ñ‚ Ğ¶Ğµ Ñ„Ğ°Ğ¹Ğ» Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾). append â€” ÑĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ»Ğ¸ĞºĞ¸ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾).",
    )

    if st.button("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ² Ğ‘Ğ”", type="primary"):
        with st.spinner("Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ±Ğ°Ğ·Ñƒ..."):
            if clicks_file:
                st.write("ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ clicks...")
                load_clicks(clicks_file, mode=load_mode)
                st.write("âœ… clicks Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")

            if conv_file:
                st.write("ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ conversions...")
                load_conversions(conv_file)
                st.write("âœ… conversions Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹")

        st.success("ğŸ‰ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ² Ğ‘Ğ”")


# ===================== Data for dashboard =====================
df = pd.read_sql(
    """
    with keys as (
      select day, subid from fact_clicks_daily
      union
      select day, subid from fact_conversions_daily
    )
    select
      k.day,
      k.subid,
      coalesce(c.clicks, 0) as clicks,
      coalesce(v.leads, 0) as leads,
      coalesce(v.sales, 0) as sales,
      d.offer,
      d.country_flag,
      d.os,
      d.sub_id_2,
      d.campaign,
      d.sub_id_1
    from keys k
    left join fact_clicks_daily c
      on c.day = k.day and c.subid = k.subid
    left join fact_conversions_daily v
      on v.day = k.day and v.subid = k.subid
    left join dim_subid d
      on d.subid = k.subid
    order by k.day;
    """,
    engine,
)

if df.empty:
    st.info("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸ CSV Ñ„Ğ°Ğ¹Ğ»Ñ‹ â€” Ğ¿Ğ¾ÑĞ²Ğ¸Ñ‚ÑÑ Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´.")
    st.stop()

# ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ·Ğ¾Ğ²
df["sub_id_2"] = df["sub_id_2"].fillna("").astype(str).str.strip()
df["sub2_norm"] = df["sub_id_2"].replace({"": "Organic"})

df["campaign"] = df["campaign"].fillna("").astype(str)
df["campaign_short"] = df["campaign"].str.split("[", n=1).str[0].str.strip()

df["offer"] = df["offer"].fillna("").astype(str).str.strip()

# ĞŸĞµÑ€Ğ¸Ğ¾Ğ´Ñ‹: Ğ²Ñ‡ĞµÑ€Ğ° / Ğ¿Ğ¾Ğ·Ğ°Ğ²Ñ‡ĞµÑ€Ğ°
today = dt.date.today()
yday = today - dt.timedelta(days=1)
pday = today - dt.timedelta(days=2)

df_y = df[df["day"] == yday].copy()
df_p = df[df["day"] == pday].copy()

y_clicks = int(df_y["clicks"].sum())
p_clicks = int(df_p["clicks"].sum())

y_leads = int(df_y["leads"].sum())
p_leads = int(df_p["leads"].sum())

y_sales = int(df_y["sales"].sum())
p_sales = int(df_p["sales"].sum())

# KPI
k1, k2, k3 = st.columns(3)
with k1:
    metric_with_pct("Ğ˜Ğ½ÑÑ‚Ğ°Ğ»Ğ»Ñ‹", y_clicks, p_clicks)  # ĞºĞ»Ğ¸ĞºĞ¸ = Ğ¸Ğ½ÑÑ‚Ğ°Ğ»Ğ»Ñ‹ (ĞºĞ°Ğº Ñ‚Ñ‹ Ñ…Ğ¾Ñ‚ĞµĞ»)
with k2:
    metric_with_pct("Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸", y_leads, p_leads)
with k3:
    metric_with_pct("ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸", y_sales, p_sales)

# Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶ Ğ¿Ğ¾ Ğ´Ğ½ÑĞ¼ (Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼)
st.subheader("ğŸ“ˆ ĞŸÑ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸ Ğ¿Ğ¾ Ğ´Ğ½ÑĞ¼")
st.line_chart(df.groupby("day")["sales"].sum())

# ===================== Top tables =====================
# Top 5 Sub ID 2 by Sales (exclude Organic)
st.subheader("ğŸ† Ğ¢Ğ¾Ğ¿ 5 Sub ID 2 Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ°Ğ¼ (Ğ²Ñ‡ĞµÑ€Ğ°)")

df_y_non_org = df_y[df_y["sub2_norm"] != "Organic"].copy()
df_p_non_org = df_p[df_p["sub2_norm"] != "Organic"].copy()

top_sub2_y = df_y_non_org.groupby("sub2_norm")["sales"].sum().sort_values(ascending=False).head(5)
top_sub2_p = df_p_non_org.groupby("sub2_norm")["sales"].sum()

rows = []
for sub2, s_y in top_sub2_y.items():
    s_p = float(top_sub2_p.get(sub2, 0))
    ch = pct_change(float(s_y), float(s_p))
    rows.append({"Sub ID 2": sub2, "Sales (yday)": int(s_y), "Î”% vs prev": ch})

df_tbl = pd.DataFrame(rows)
sty = (
    df_tbl.style
    .format({"Î”% vs prev": fmt_pct_cell})
    .applymap(style_pct_color, subset=["Î”% vs prev"])
)
st.dataframe(sty, use_container_width=True)

# Top 5 Campaign by Sales (campaign_short)
st.subheader("ğŸ† Ğ¢Ğ¾Ğ¿ 5 ĞšĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ°Ğ¼ (Ğ²Ñ‡ĞµÑ€Ğ°)")

top_c_y = df_y.groupby("campaign_short")["sales"].sum().sort_values(ascending=False).head(5)
top_c_p = df_p.groupby("campaign_short")["sales"].sum()

rows = []
for camp, s_y in top_c_y.items():
    s_p = float(top_c_p.get(camp, 0))
    ch = pct_change(float(s_y), float(s_p))
    rows.append({"Campaign": camp, "Sales (yday)": int(s_y), "Î”% vs prev": ch})

df_tbl = pd.DataFrame(rows)
sty = (
    df_tbl.style
    .format({"Î”% vs prev": fmt_pct_cell})
    .applymap(style_pct_color, subset=["Î”% vs prev"])
)
st.dataframe(sty, use_container_width=True)


# ===================== Gainers (% + NEW) =====================
def gain_table_pct(group_col: str, metric_col: str, title: str, top_n: int = 10, exclude_organic: bool = False):
    st.subheader(title)

    a = df_y.copy()
    b = df_p.copy()

    if exclude_organic and group_col == "sub2_norm":
        a = a[a["sub2_norm"] != "Organic"]
        b = b[b["sub2_norm"] != "Organic"]

    y = a.groupby(group_col)[metric_col].sum()
    p = b.groupby(group_col)[metric_col].sum()

    idx = y.index.union(p.index)
    out = pd.DataFrame(
        {
            group_col: idx,
            f"{metric_col} (yday)": y.reindex(idx, fill_value=0).astype(int).values,
            f"{metric_col} (prev)": p.reindex(idx, fill_value=0).astype(int).values,
        }
    )

    prev_vals = out[f"{metric_col} (prev)"].astype(float)
    yday_vals = out[f"{metric_col} (yday)"].astype(float)

    out["Î”% vs prev"] = None
    mask = prev_vals > 0
    out.loc[mask, "Î”% vs prev"] = ((yday_vals[mask] - prev_vals[mask]) / prev_vals[mask]) * 100.0

    # Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ¾ÑÑ‚
    out = out[out["Î”% vs prev"].notna()]
    out = out[out["Î”% vs prev"] > 0]

    # ÑĞºĞ¾Ñ€Ğ¸Ğ½Ğ³: % * Ğ¾Ğ±ÑŠÑ‘Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ²Ñ‹Ğ»ĞµÑ‚Ğ°Ğ»Ğ¸ â€œ+500% Ğ¾Ñ‚ 1â€
    out["_score"] = out["Î”% vs prev"].astype(float) * out[f"{metric_col} (yday)"].astype(float)
    out = out.sort_values(["_score", f"{metric_col} (yday)"], ascending=False).head(top_n)
    out = out.drop(columns=["_score"])

    sty = (
        out.style
        .format({"Î”% vs prev": fmt_pct_cell})
        .applymap(style_pct_color, subset=["Î”% vs prev"])
    )
    st.dataframe(sty, use_container_width=True)


def new_table(group_col: str, metric_col: str, title: str, top_n: int = 10, exclude_organic: bool = False):
    st.subheader(title)

    a = df_y.copy()
    b = df_p.copy()

    if exclude_organic and group_col == "sub2_norm":
        a = a[a["sub2_norm"] != "Organic"]
        b = b[b["sub2_norm"] != "Organic"]

    y = a.groupby(group_col)[metric_col].sum()
    p = b.groupby(group_col)[metric_col].sum()

    idx = y.index.union(p.index)
    out = pd.DataFrame(
        {
            group_col: idx,
            f"{metric_col} (yday)": y.reindex(idx, fill_value=0).astype(int).values,
            f"{metric_col} (prev)": p.reindex(idx, fill_value=0).astype(int).values,
        }
    )

    out = out[(out[f"{metric_col} (yday)"] > 0) & (out[f"{metric_col} (prev)"] == 0)]
    out = out.sort_values(f"{metric_col} (yday)", ascending=False).head(top_n)

    st.dataframe(out, use_container_width=True)


# Traffic gainers (clicks)
gain_table_pct("sub2_norm", "clicks", "ğŸ“ˆ Top 10 Sub ID 2 Traffic Gainers (ĞºĞ»Ğ¸ĞºĞ¸, %)", exclude_organic=True)
new_table("sub2_norm", "clicks", "ğŸ†• New Sub ID 2 Traffic (prev=0)", exclude_organic=True)

gain_table_pct("campaign_short", "clicks", "ğŸ“ˆ Top 10 ĞšĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Traffic Gainers (ĞºĞ»Ğ¸ĞºĞ¸, %)")
new_table("campaign_short", "clicks", "ğŸ†• New ĞšĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Traffic (prev=0)")

# Sales gainers
gain_table_pct("sub2_norm", "sales", "ğŸ’° Top 10 Sub ID 2 Sales Gainers (Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸, %)", exclude_organic=True)
new_table("sub2_norm", "sales", "ğŸ†• New Sub ID 2 Sales (prev=0)", exclude_organic=True)

gain_table_pct("campaign_short", "sales", "ğŸ’° Top 10 ĞšĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Sales Gainers (Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸, %)")
new_table("campaign_short", "sales", "ğŸ†• New ĞšĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Sales (prev=0)")

gain_table_pct("offer", "sales", "ğŸ’° Top 10 ĞÑ„Ñ„ĞµÑ€ Sales Gainers (Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸, %)")
new_table("offer", "sales", "ğŸ†• New ĞÑ„Ñ„ĞµÑ€ Sales (prev=0)")
